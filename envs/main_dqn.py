import numpy as np
from rlss_envs import ServerlessEnv 
from dqn_agent import Agent as dqn
from gymnasium import spaces
import matplotlib.pyplot as plt
import argparse
import torch


def main(args):
    # Environment variable
    num_service = 2
    log_file = "log.txt"

    # DQN_agent
    episodes = 10000                        # Total episodes for the training
    batch_size = 32                        # Total used memory in memory replay mode
    max_env_steps = 100                    # Max steps per episode
    batch_update = 4

    env_config = {"render_mode": None, 
                  "size": num_service, 
                  "log_file": log_file}
    
    env = ServerlessEnv(env_config=env_config)

    action_size = env.action_size
    print('action_size: {}'.format(action_size))

    state_size = env.state_space.shape[0]
    print('state_size: {}'.format(state_size))

    # Training the model

    # Exploration initiation
    eps = 1.
    eps_end = 0.01
    eps_decay = 0.995

    # Instantiating the DQN_Agent
    agent = dqn(state_size, action_size)

    tab = {}
    avg_reward_list = []
    rewards = []
    cumulative_rewards = []
    
    for e in range(episodes):
        state = env.reset()
        state = np.reshape(state, [state_size])
        done = False
        cum_reward = 0
        while not done:
            action = agent.get_action(state=state,env=env)
            next_state, reward, done, _ = env.step(action)
            env.render()
            tab[e * env.current_time + env.current_time] = {"action": action, "reward": reward, "next_state": next_state}
            next_state = np.reshape(next_state, [state_size])
            agent.store_transition(state, action, reward, next_state, done)
            state = next_state
            agent.learn()
            rewards.append(reward)
            cum_reward += reward
        # Saving the Model's weights generated by the selected model
        if e % batch_update == 0:
            agent.update_target_network()
            # Saving the Model's weights generated by the selected model
            agent.save_models()
        eps = max(eps_end, eps_decay*eps)
        cumulative_rewards.append(cum_reward)
        with open(log_file, 'a') as f:
            f.write("Episode: {}/{}, sub_episode: {}, reward: {}\n".format(e, episodes, env.current_time, cum_reward))
        if e > 100:
            avg = np.mean(cumulative_rewards[-100:])
        else:
            avg = np.mean(cumulative_rewards)
        avg_reward_list.append(avg)
        plt.figure(2)
        plt.clf()
        rewards_t = torch.tensor(cumulative_rewards, dtype=torch.float)
        plt.title('Training...')
        plt.xlabel('Episode')
        plt.ylabel('Cumulative reward')
        plt.grid(True)
        plt.plot(rewards_t.numpy())
        # Take 100 episode averages and plot them too
        if len(rewards_t) >= 100:
            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)
            means = torch.cat((torch.zeros(99), means))
            plt.plot(means.numpy())
        plt.pause(0.001)  # pause a bit so that plots are updated
        plt.savefig('{}/live_average_rewards_DQN_3mec_10vnf.png'.format(args['file']))
        plt.close()
        agent.save_models()
    # Plotting the reward/avg_reward
    if args['train'] is not None:
        plt.plot((np.arange(len(avg_reward_list)) + 1), avg_reward_list)
        plt.xlabel('Episodes')
        plt.ylabel('Average Reward')
        plt.title('Average Reward vs Episodes')
        plt.savefig('{}/average_rewards_dqn_{}.png'.format(args['file'], args['train']))
        plt.close()

        plt.plot(cumulative_rewards)
        plt.plot(avg_reward_list)
        plt.legend(["Reward", "100-episode average"])
        plt.title("Reward history")
        plt.savefig('{}/live_average_rewards_dqn_{}_final.png'.format(args['file'], args['train']))
        plt.close()

        # Saving all sort of statistics
        with open("action_state_information_{}.txt".format(args['train']), "a") as w:
            w.write(str(tab))

        with open("detailed_action_selection_{}.txt".format(args['train']), "a") as w:
            w.write(str(agent.action))
    with open("{}/reward_list_dqn_{}.txt".format(args['file'], args['train']), "w") as w:
        w.write(str(rewards))
    with open("{}/average_reward_list_dqn_{}.txt".format(args['file'], args['train']), "w") as w:
        w.write(str(avg_reward_list))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parsing the type of DRL/RL to be tested')
    parser.add_argument('-t', '--train', help='Train DRL/RL', required=True)
    parser.add_argument('-o', '--observe', help='Observe a trained DRL/RL')
    parser.add_argument('-f', '--file', help='file name')
    args = vars(parser.parse_args())
    main(args)